---
title: '5 参数估计'
date: 2025-07-10T16:07:24+08:00
math: true
---

## 教学目的：

1) 让学生理解矩估计和极大似然估计方法

2) 理解置信区间定义

3) 掌握常见的总体分布下参数的点估计和置信区间的计算.

设有一个总体，以 $f(x;\theta_{1},\cdots,\theta_{k})$ 记其概率密度函数 (若总体分布是连续性的)，或其概率函数 (若总体分布为离散型的)．为叙述方便我们统一称 $f(x;\theta_{1},\cdots,\theta_{k})$ 为总体的概率函数，参数估计问题是利用从总体抽样得到的信息来估计总体的某些参数或者参数的某些函数．一般假定总体分布形式已知，未知的仅仅是一个或几个参数．利用从总体 $f(x;\theta_{1},\cdots,\theta_{k})$ 中抽取的一组样本 $X_1$ X $X_{1},\cdots,X_{n}$ Xn $X_n$ 去对参数 $:\theta_1$ 01 $\theta_{1},\cdots,\theta_{k}$ 0k $\theta_{k}$ 的未知值作出估计或估计它们的某个已知函数 $g(\theta_{1},\cdots,\theta_{k})$

## 5.1 点估计

设总体 $X$ 的分布函数形式已知，但它的一个或多个参数为未知，例如参数 9 未知，根据样本 X1 $X_1$ $X_{1},\cdots,X_{n}$ Xn $X_n$ 来估计参数 $\theta$ ，就是要构造适当的统计量 ${\hat{\theta}}={\hat{\theta}}(X_{1},\cdots,X_{n})$ ．当有了样本 X1 $X_1$ $X_{1},\cdots,X_{n}$ Xn $X_n$ 的值后，就代入 $\hat{\theta}=\hat{\theta}(X_{1},\cdots,X_{n})$ 中算出一个值，用来作为 0 的估计值. 为这样特定目的而构造的统计量 $\hat{\theta}$ 叫做 9 的估计量. 由于参数 $\theta$ 是数轴上的一个点，用 $\hat{\theta}$ 估计 $\cdot\theta$ 等于用一个点去估计另一个点，所以这样的估计叫做点估计，

求点估计的方法有多种，下面介绍两种点估计方法

### 5.1.1 矩估计方法

矩方法追溯到 19 世纪的 KarlPearson，矩方法是基于一种简单的“替换 " 思想建立起来的一种估计方法，其基本思想是用样本矩估计总体矩，由大数律，如果未知参数和总体的某个 (些) 矩有关系，我们很自然的来构造未知参数的估计。

回忆一下以前关于矩的记法：

$$
\text{样本}k\text{阶矩:}\quad a_k=\frac{1}{n}\sum_{i=1}^nX_i^k\quad m_k=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^k
$$

$$
\alpha_k=EX^k\quad\mu_k=E(X-EX)^2
$$

因此在 k 阶矩存在的情况下，根据大数律有

$$
\begin{matrix}a_k\xrightarrow{p}\alpha_k,&m_k\xrightarrow{p}\mu_k\end{matrix}
$$

从而我们可以使用 $a_k,m_k$ 分别估计 $\alpha_k,\mu_k\circ$ 介绍如下：假设总体 $X$ 包含 k 个未知参数 $\theta_{1},\cdots,\theta_{k}$, 由方程组

$$
\left\{\begin{array}{c}\alpha_1=f_1(\theta_1,\cdots,\theta_k)\\\vdots\\\alpha_k=f_k(\theta_1,\cdots,\theta_k)\end{array}\right.
$$

反解得到

$$
\left\{\begin{array}{c}\theta_1=g_1(\alpha_1,\cdots,\alpha_k)\\\vdots\\\theta_k=g_k(\alpha_1,\cdots,\alpha_k)\end{array}\right.
$$

将其中的总体矩用相应的样本矩代替，则我们可以得到参数 ${:}\theta_{1},\cdots,\theta_{k}$ 的一个估计

$$
\left\{\begin{array}{c}\hat{\theta}_1=g_1(a_1,\cdots,a_k)\\\vdots\\\hat{\theta}_k=g_k(a_1,\cdots,a_k)\end{array}\right.
$$

若要估计参数 $\theta_{1},\cdots,\theta_{k}$ 0k $\theta_{k}$ 的某函数 $g(\theta_{1},\cdots,\theta_{k})$ ，则用 $g(\hat{\theta}_{1},\cdots,\hat{\theta}_{k})$ 去估计它

这里我们用的都是原点矩 $\vdots\alpha_{k}$ ，当然也可以使用中心矩 $\mu_k$ ，或者两个都使用。在这和情况下，只需要把相应的总体矩换成样本矩。我们称这种估计方法为矩估计法，得到的估计量称为矩估计量。矩估计方法应用的原则是：能用低阶矩处理的就不用高阶矩。

矩估计法的优点是简单易行，并不需要事先知道总体是什么分布. 缺点是，当总体类型已知时，没有充分利用分布提供的信息. 一般场合下，矩估计量不具有唯一性

例 5.1.1. 投掷一枚硬币，为了解正面出现的概率，现独立重复的投掷 $:n$ 次，用 $X_{1},\cdots,X_{n}$ 表示投掷结果. 显然此时总体 $X$ 的分布为 $B(1,p)$ ， $p$ 为感兴趣的量. 而 $X_{1},\cdots,X_{n}$ Xn $X_n$ 为样本，则求参数 $p$ 的矩估计量。

解：由于 $EX=p$ ，而样本均值 $X$ 收敛到总体均值 $EX$ ，因此 $p$ 的一个矩估计量为 $\hat{p}=\bar{X}$

例 5.1.2. 为考察某种考试成绩分布情况，使用正态分布 $N(a,\sigma^{2})$ 来作为总体 $X$ 的分布. 现在从中随机调查 $n$ 个人，即样本为 $X_{1},\cdots,X_{n}$ .试求参数 $a,\sigma^2$ 的矩估计量。

解：由于

$$
EX=a,\quad Var(X)=\sigma^2
$$

所以 $a,\sigma^{2}$ 的一个矩估计量为

$$
\hat{a}=\bar{X},\quad\hat{\sigma}^2=m_2=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2
$$

我们知道 $ES^{2}=\sigma^{2}$ ，因此， $\sigma^{2}$ 的另一个矩估计量为 $\hat{\sigma}^{2}=S^{2}$

### 5.1.2 极大似然估计方法

极大似然方法到目前为止应用最广的的点估计方法. 这种方法是基于如下的看法

**定义 5.1.1.** 设总体 $X$ 有概率函数 $f(x;\theta)=f(x;\theta_{1},\cdots,\theta_{k})$ ，这里参数 $\theta=(\theta_{1},\cdots$ $\theta_k)\in\Theta$ ，而当固定 $x$ 时把 $f(x;\theta)$ 看成为 $\theta$ 的函数，称为似然函数，常记为 $L(x;\theta)$ 或 $.L(\theta)$

当固定参数 9 时， $f(x;\theta)$ 可以看成是得到样本观察值 $x$ 的可能性，这样，当把参数 $:\theta$ 看成变动时，也就得到“在不同的 9 值下能观察到 $|x$ 的可能性大小，即 $L(x;\theta)$ ”；由于我们已经观察到了 $x$ ，所以我们要寻求在哪一个 9 的值下，使得能观察到 $x$ 的可能性 $L(x;\theta)$ 最大。这个 9 的值即称为 9 极大似然估计值（看上去最有可能的)。我们先看一个例子：

例 5.1.3. 从鱼池里随机捕捞 500 条鱼，做好记号后重新放入鱼池中，待充分混合后再捕捞 1000 条鱼，结果发现其中有 72 条带有记号，试问鱼池中可能有多少条鱼

解：先将问题一般化. 设池中有 V 条鱼，其中 r 条做好记号. 随机捕捞 s 条，发现 r 条有记号. 用上述信息来估计 N.

用 $X$ 表示捕捞的 s 条鱼中带记号鱼的数目，则

$$
P(X=x)=\frac{C_{N-r}^{s-x}C_{r}^{x}}{C_{N}^{s}}.
$$

目前发现在捕捞的 s 条鱼中有记号的鱼 $x$ 条，要寻求 $N$ 取何值时，使得观察到这个事件 $\{X=$ $x\}$ 的可能性最大. 即 $x$ 是固定的， $N$ 是变化的，记 $p(x;N)=P(X=x)$ ．因为

$$
g(N):=\frac{p(x;N)}{p(x;N-1)}=\frac{(N-s)(N-r)}{N(N-r-s+x)}=\frac{N^2-N(s+r)+rs}{N^2-N(r+s)+Nx},
$$

当 $rs>Nx$ 时， $g(N)>1$ : $rs<Nx$ 时， $g(N)<1$ 所以 $P(X=x)$ 在 $N=\frac{rs}x$ 附近达到最大，注意到 $N$ 只能取正整数，故 $N$ 的最可能的估计即极大似然估计为

$$
\hat{N}=\left\lceil\frac{rs}{x}\right\rceil.
$$

其中「表示下取整，即小于该值的最大整数. 将题目中的数字代入

$$
\hat{N}=\left\lceil\frac{500\times1000}{72}\right\rceil=6944.
$$

即鱼池中的总的鱼数为 6694 条

现给出极大似然估计的一般性定义

**定义 5.1.2.** 设 $X=(X_{1},\cdots,X_{n})$ 为从具有概率函数 f 的总体中抽取的样本， $\theta$ 为未知参数或者参数向量. $x=(x_1,\cdots,x_n)$ 为样本的观察值。若在给定 $x$ 时，值 0=6（x）满足下式

$$
\begin{aligned}L(\hat\theta)=\max_{\theta\in\Theta}L(x;\theta)\end{aligned}
$$

则称 $\hat{\theta}$ 为参数 $\theta$ 的极大似然估计值，而 $\hat{\theta}(X)$ 称为参数 $\theta$ 的极大似然估计量。若待估参数为 0 的函数 $g(\theta)$ ，则 $g(\theta)$ 的极大似然估计量为 $g(\hat{\theta})$

求极大似然估计值相当于求似然函数的最大值。在简单样本的情况下，

$$
L(x;\theta)=\prod_{i=1}^nf(x_i;\theta)
$$

而把似然函数的对数 $l(\theta)=\log L(\theta)$ 称为对数似然函数 (这是由于在一些情况下，处理对数似然函数更方便

当似然函数对变量 9 单调时，我们可以容易得到其最大值点. 反之当似然函数为非单调函数且对变量 $\theta$ 可微分时，我们可以求其驻点：令

$$
\frac{dl(\theta)}{d\theta}=0\quad(\text{或者}\frac{dL(\theta)}{d\theta}=0)
$$

当 0 为多维时，比如 $\theta=(\theta_{1},\cdots,\theta_{k})$ 时令

$$
\frac{\partial l(\theta)}{\partial\theta_i}=0\quad(\text{或者}\frac{\partial L(\theta)}{\partial\theta_i}=0)\:i=1,\cdots,k
$$

然后判断此驻点是否是最大值点。

例 5.1.4. 设 $X_{1},\cdots,X_{n}$ 为从总体 $X\sim N(a,\sigma^{2})$ 中抽取的样本，求参数 a， $\sigma^{2}$ 的极大似然估计量。

解：易得对数似然函数为

$$
l(a,\sigma^2)=c-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-a)^2-\frac{n}{2}log(\sigma^2)
$$

其中 c 是与参数无关的常数. 令

$$
\left\{\begin{array}{c}\frac{\partial l(a,\sigma^2)}{\partial a}=0\\\frac{\partial l(a,\sigma^2)}{\partial\sigma^2}=0\end{array}\right.
$$

得到

$$
\left\{\begin{array}{l}a=\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i\\\sigma^2=\frac{1}{n}\sum_{i=1}^n(x_i-a)^2\end{array}\right.
$$

容易验证此驻点是唯一的最大值点，因此得到 $a,\sigma^{2}$ 的极大似然估计量

$$
\left\{\begin{array}{ll}\hat{a}=\bar{X}\\[2ex]\hat{\sigma}^2=\frac{1}{n}\sum\limits_{i=1}^n(X_i-\bar{X})^2.\end{array}\right.
$$

有时函数 $f$ 并不对 01.·.， $\theta_{k}$ 可导，甚至 f 本身也不连续，这时求导就没法用，必须回到原始定义，

例 5.1.5. 设总体 $X$ 服从 $[a,b]$ 上的均匀分布， $a<b$, 求参数 $a,t$ 的极大似然估计

解：易得似然函数为

$$
\begin{aligned}L(a,b)=\frac{1}{(b-a)^n}\prod_{j=1}^nI(a\leq x_j\leq b)=\frac{1}{(b-a)^n}I(a\leq x_{(1)}\leq x_{(n)}\leq b).\end{aligned}
$$

于是对任何满足条件 $a\leq x_{i}\leq b$ 的 $a,b$ 都有

$$
L(a,b)=\frac{1}{(b-a)^n}\leq\frac{1}{(x_{(n)}-x_{(1)})^n},
$$

即似然函数 $L(a,b)$ 在 $a$ = $x_{( 1) }, b$ = $x_{( n) }$ 时取到最大值. 于是 $a,b$ 的极大似然估计量为 $\hat{a}=$

$$
X_{(1)},\hat{b}=X_{(n)}.
$$

例 5.1.6. 设 $X_{1},\cdots,X_{n}$ 为从具有如下形式密度的总体中抽取的样本：

$$
f(x;a,b)=\left\{\begin{array}{cc}\frac{1}{b}\exp\{-\frac{x-a}{b}\}&,x>a\\0&,x\leq a\end{array}\right.
$$

求参数 $a,t$ 的极大似然估计量，

解：易得似然函数为

$$
L(a,b)=\prod\limits_{i=1}^nf(x_i;a,b)=\frac{1}{b^n}\exp\{-\frac{1}{b}\sum\limits_{i=1}^n(x_i-a)\}I(x_{(1)}>a)
$$

在固定 6 时，显然似然函数为 $a$ 的单调增函数，因此 ${:}L(a)$ 的驻点为 $\hat{a}=x_{(1)}$ 。再令 $\frac{\partial L(a,b)}{\partial b}=0$ 得到 b=(c－2 (1)，容易验证此解是最大值点。从而得到 $|a,b$ 的极大似然估计量：

$$
\left\{\begin{array}{ll}\hat{a}=X_{(1)}\\\hat{b}=\frac{1}{n}\sum\limits_{i=1}^n(X_i-X_{(1)}).\end{array}\right.
$$

例 5.1.7. 设 $X_{1},\cdots,X_{n}$ 为从如下分布中抽取的简单样本，求 0 的极大似然估计

$$
f(x)=\frac{1}{x!(2-x)!}[\theta^x(1-\theta)^{2-x}+\theta^{2-x}(1-\theta)^x],\quad x=0,1,2;\quad\theta\in(0,\frac{1}{2})
$$

解：由题设知 $f(x)$ 为离散型，其分布律为

X

0

1

2

P

$\frac{1}{2}[(1-\theta)^{2}+\theta^{2}]$

$2\theta(1-\theta)$

$\frac{1}{2}[(1-\theta)^{2}+\theta^{2}]$

若直接从此分布出发，则不能得到 9 的极大似然估计的显式表达。为此，我们重新参数化，记 $\eta=2\theta (1-\theta)$ ．则由题设知 $\eta<1/2$ 。则

X | 0 | 1 | 2

P | $\frac{1}{2}(1-\eta)$ | $\eta$ | $\frac{1}{2}(1-\eta)$

再记 $n_{i}=\#\{X_{1},\cdots, X_{n}$ X $X_n$ 中等于的个数}， $i=0,1,2$ ，则得到似然函数为

$$
L(\eta)=(\frac{1}{2}(1-\eta))^{n_0}\eta^{n_1}(\frac{1}{2}(1-\eta))^{n_2}=(\frac{1}{2}(1-\eta))^{n-n_1}\eta^{n_1}
$$

求解并注意 $\eta$ 的上界即得到 $\eta$ 的极大似然估计为

$$
\hat{\eta}=\max\{\frac{n_1}{n},\frac{1}{2}\}
$$

再由 $\theta={\frac{1-\sqrt{1-2\eta}}{2}}$ 得到的极大似然估计为

$$
\hat\theta=\frac{1-\sqrt{1-2\hat\eta}}{2}
$$

### 5.1.3 点估计的优良准则

我们看到对同一个参数，有多个不同的估计量，因此，评选不同估计量的优劣性是需要考虑的。

### 1. 无偏性

设 ${:}\hat{g}(X_{1},\cdots, X_{n})$ 为待估参数函数 $g (\theta)$ 的一个估计量，若

$$
E\hat{g}(X_1,\cdots,X_n)=g(\theta)
$$

则称 $\hat{g}(X_{1},\cdots, X_{n})$ 为 $g (\theta)$ 的无偏估计量。无偏性是对一个估计量的最基本的要求，其实际意义就是无系统误差. 因此在有多个估计量可供选择时，我们优先考虑无偏估计量。

很多时候我们得到的估计量是有偏，例如正态总体的方差 $\sigma^2$ 的极大似然估计量 $\hat{\sigma}^2=$ (X-X) 2 是有偏的， $E\hat{\sigma}^{2}=\frac{n-1}{n}\sigma^{2}.$ 若以 $\frac n{n-1}$ 乘以 $,\hat{\sigma}^{2}$ ，所得到的估计量就是无偏的. 这种方法称为修正

若某一参数存在多个无偏估计时，如何来选择使用哪个估计量？人们又在无偏性的基础上增加了对方差的要求

### 2. 有效性

设 ${:}\hat{g}_{1}(X_{1},\cdots, X_{n})$ 和 $\hat{g}_{2}(X_{1},\cdots, X_{n})$ 为待估参数函数 $g (\theta)$ 的两个不同的无偏估计量，若对任意的 $\theta\in\Theta$ 有

$$
Var(\hat{g}_{1}(X_{1},\cdots,X_{n}))\leq Var(\hat{g}_{2}(X_{1},\cdots,X_{n}))
$$

而且至少对某个 $\theta_0\in\Theta$ 使得严格不等式成立。则称 $\hat{g}_{1}$ 较 $\hat{g}_{2}$ 有效。

### 3. 相合性

设总体分布依赖于参数 $\theta_{1},\cdots,\theta_{k}, g (\theta_{1},\cdots,\theta_{k})$ 是待估参数函数。设 $X_{1},\cdots, X_{n}$ 为自该总体中抽取的样本， $T (X_{1},\cdots, X_{n})$ 为 $g (\theta_{1},\cdots,\theta_{k})$ 的一个估计量，如果对任意的 e> 0 和 $|\theta_{1},\cdots,\theta_{k}$ 0k $\theta_{k}$ 的一切可能值都有

$$
\lim\limits_{n\to\infty}P_{\theta_1,\cdots,\theta_k}(|T(X_1,\cdots,X_n)-g(\theta_1,\cdots,\theta_k)|\geq\epsilon)=0
$$

我们则称 $T (X_{1},\cdots, X_{n})$ 为 $g (\theta_{1},\cdots,\theta_{k})$ 的一个 (弱) 相合估计量。

相合性是对一个估计量的最基本的要求，如果一个估计量没有相合性，那么无论样本大小多大，我们也不能把未知参数估计到任意预定的精度。这种估计量显然是不可取的。

矩估计量是满足相合性的，极大似然估计量在很一般的条件下也是满足相合性的。

### 4. 渐近正态性

估计量是样本 $X_{1},\cdots, X_{n}$ Xn $X_n$ 的函数，其确切的分布一般不是容易得到。但是，许多形式很复杂的统计量（未必是和)，当 $n$ 很大时，其分布都渐近于正态分布，这个性质称为统计量的“渐近正态性”。

无偏性和有效性都是对固定的样本大小 $\cdot n$ 而言的，这种性质称为估计量的“小样本性质”，而相合性和渐近正态性都是考虑在样本大小趋于无穷时的性质，这种性质称为“大样本性质 "。

例 5.1.8. 设从总体

| X | 0 | 1/2 | 2 | 3 |
|---|-----|-----|------|-------|
| P | θ/2 | θ/2 | 3θ/2 | 1-3θ |

抽取的一个简单样本 X1 $X_1$ $X_{1},\cdots, X_{10}$ X10 $X_{10}$ 的观察值为 (0,3,1,1,0,2,0,0,3,0) ，

(1）求 0 的矩估计量 $\hat{\theta}_{M}$ 和极大似然估计量 $\hat{\theta}_{L}$ ，并求出估计值。

(2) 上述估计量是否为无偏的？若不是，请作修正， (3）比较修正后的两个估计量，指出那个更有效，

由有效性的定义，我们自然会问在一切可能的无偏估计里，能否找到具有最小方差的无偏估计量？如果存在这样的估计量，我们称其为最少方差无偏估计量，详细地可参考课本。

## 5.2 区间估计

对于一个未知量，人们在测量和计算时，常不以得到近似值为满足，还需要估计误差及要求知道近似值的精确程度（亦即所求真值所在的范围）. 类似的，对于未知的参数 9. 除了求出它的点估计 6 外. 我们还希望估计出一个范围. 并希望知道这个范围包含参数 9 真值得可信程度. 这样的范围通常以区间形式给出，同时还给出此区间包含真值得可信程度. 这种形式的估计称为区间估计.

比如一个人的年龄在 18-25 之间：月支出在 400-600 元之间等. 区间估计的好处是把可能的误差用醒目的形式表示出来了，比如你估计月花费支出是 500. 我们相信多少会有误差，但是误差有多大？单从你提出的 500 这个数字还给不出什么信息，若你给出估计支

出是 400-600 之间，则人们相信你在作出这估计时，已把可能出现的误差考虑到了，多少给人们以更大的信任感. 因此区间估计也是常用的一种估计方式

现在最流行的一种区间估计理论是 J.Neyman 在上世纪 30 年代建立起来的．他的理论的基本概念很简单，为表达方便，我们暂时假定总体分布只包含一个未知参数 $:\theta$ ，且要估计的就是 9 本身. 如果总体分布中包含若干位置参数 01 $\theta_1$ $\theta_{1},\cdots,\theta_{k}$, 而要估计的是 $g (\theta_{1},\cdots,\theta_{k})$ 则基本概念和方法并无不同. 这在后面的例子里可以看出.

### 5.2.1 置信区间

Neyman 建立起来的区间估计也叫置信区间，字面上的意思是：对该区间能包含未知参数 9 可置信到何种程度. 假设 $X_{1},\cdots, X_{n}$ Xn $X_n$ 是从该总体中抽取的样本，所谓 0 的区间估计，就是要寻求满足条件 $\underline{\theta}(X_{1},\cdots, X_{n})<\bar{\theta}(X_{1},\cdots, X_{n})$ 的两个统计量 9 和 9 为端点的区间 $[\underline{\theta},\bar{\theta}]$ 一旦有个样本 $X_{1},\cdots, X_{n}$ Xn $X_n$ 的值后，就把 $\vdots\theta$ 估计在区间 $[\underline{\theta}(X_{1},\cdots, X_{n}),\bar{\theta}(X_{1},\cdots, X_{n})]$ 之内. 不难理解，这里有两个要求

. $\theta$ 以很大概率被包含在区间 $[\underline{\theta},\bar{\theta}]$ 内，也就是说

$$
P_\theta(\underline{\theta}\leq\theta\leq\bar{\theta})=1-\alpha
$$

尽可能大，即要求估计尽量可靠

·估计的精度要尽可能高，比如要求区间 $[\underline{\theta},\bar{\theta}]$ 要尽可能的短，或者某种能体现这个要求的其他准则。

比如估计一个人的年龄，如 [30.35]，我们自然希望这个人的年龄有很大把握在这个区间之内，并且希望这个区间不能太长. 如果估计是 [10.90]，当然可靠了，但是精度太差，用处不大

但这两个要求是相互矛盾的，因此区间估计的原则是在已有的样本资源限制下，找出更好的估计方法以尽量提高可靠性和精度。Neyman 提出了广泛接受的准则：先保证可靠性，在此前提下尽可能提高精度。为此，引入如下定义

**定义 5.2.1.** 设总体分布 $F (x,\theta)$ 含有一个或多个未知的参数 0， $\theta\in\Theta$ ，对给定的值 $.\alpha, (0<$ $\alpha<1$ ），若由样本 $X_{1},\cdots, X_{n}$ 确定的两个统计量 $\bar{\theta}=\bar{\theta}(X_{1},\cdots, X_{n})$ 和 $\underline{\theta}=\underline{\theta}(X_{1},\cdots, X_{n})$ 满足

$$
P_\theta(\underline{\theta}\leq\theta\leq\bar{\theta})=1-\alpha\quad\forall\:\theta\in\Theta
$$

称 $1-\alpha$ 为置信系数或置信水平，而称 $[\underline{\theta},\bar{\theta}]$ 为 $\theta$ 的置信水平为 $1-\alpha$ 的置信区间。

区间估计就是在给定的置信水平之下，去寻找有优良精度的区间。

一般，我们首先寻求参数 9 的一个估计（多数是基于其充分统计量构造的)，然后基于此估计量构造参数 0 的置信区间，介绍如下：

### **1. 枢轴变量法** 设待估参数为 $g (\theta)$

1. 找一个与待估参数 $g (\theta)$ 有关的统计量 T，一般是其一个良好的点估计（多数是通过极大似然估计构造

2. 设法找出 $T$ 与 $g (\theta)$ 的某一函数 $S (T, g (\theta))$ 的分布，其分布 $F$ 要与参数 0 无关 $(S$ 即为枢轴变量);

3. 对任何常数 $a<b$ ，不等式 $a\leq S (T, g (\theta))\leq b$ 要能表示成等价的形式 $A\leq g (\theta)\leq B$ ，其中 $A, B$ 只与 $T, a, b$ 有关而与参数无关 4. 取分布 $F$ 的上 $.\alpha/2$ 分位数 ${:}\omega_{\alpha/2}$ 和上 $(1-\alpha/2)$ 分位数 $\omega_{1-\alpha/2}$ ，有 $F ( \omega _{\alpha / 2}) - F ( \omega _{1- \alpha / 2})$ = $1-\alpha.$ 因此

$$
P(\omega_{1-\alpha/2}\leq S(T,g(\theta))\leq\omega_{\alpha/2})=1-\alpha
$$

由 3 我们就可以得到所求的置信区间

例 5.2.1. 设 $X_{1},\cdots, X_{n}$ 为从正态总体 $N (\mu,\sigma^{2})$ 中抽取得样本，求参数 $\mu,\sigma^{2}$ 的 $1-\alpha$ 置信区间。

解：由于 $\mu,\sigma^{2}$ 的估计 $X, S^{2}$ 满足

$$
\begin{gathered}
\text{T} =\sqrt{n}(\bar{X}-\mu)/S\sim t_{n-1} \\
T_{2} =(n-1)S^2/\sigma^2\sim\chi_{n-1}^2
\end{gathered}
$$

所以 $T_{1}, T_{2}$ 就是我们所要寻求的枢轴变量，从而易得参数 $\mu,\sigma^{2}$ 的 $1-a$ 置信区间分别为

$$
\left[\bar{X}-\frac{1}{\sqrt{n}}St_{n-1}(\alpha/2),\bar{X}+\frac{1}{\sqrt{n}}St_{n-1}(\alpha/2)\right],\\\left[\frac{(n-1)S^{2}}{\chi_{n-1}^{2}(\alpha/2)},\frac{(n-1)S^{2}}{\chi_{n-1}^{2}(1-\alpha/2)}\right].
$$

例 5.2.2. 设 $X_{1},\cdots, X_{n}$ 为从正态总体 $N (\mu_{1},\sigma_{1}^{2})$ 中抽取得样本， $Y_{1},\cdots, Y_{m}$ 为从正态总体 $N (\mu_{2},\sigma_{2}^{2})$ 中抽取得样本，两组样本相互独立。求参数 $\mu_{1}-\mu_{2},\sigma_{1}^{2}/\sigma_{2}^{2}$ 的 $1-\alpha$ 置信区间。

解：方法完全类似于前面的例子，此处略

### 2. 大样本法

大样本法就是利用中心极限定理，以建立枢轴变量。通过以下例子说明

例 5.2.3. 某事件 A 在每次实验中发生的概率都是 p，作 $n$ 次独立的实验，以 Yn 记 A 发生的次数。求 $. p$ 的 $1-\alpha$ 置信区间。

解：设 $n$ 比较大，令 $q=1-p$ ，则由中心极限定理知，近似有 $(Y_{n}-np)/\sqrt{npq}\sim N (0,1)$ ，从而 $(Y_{n}-np)/\sqrt{npq}$ 可以作为枢轴变量。由

$$
P(-u_{\alpha/2}\leq(Y_n-np)/\sqrt{npq}\leq u_{\alpha/2})\approx1-\alpha\quad(*)
$$

可以等价表示成

$$
P(A\leq p\leq B)\approx1-\alpha
$$

其中 A.B 为方程

$$
(Y_n-np)/\sqrt{npq}=u_{\alpha/2}
$$

的解，即

$$
A,B=\frac{n}{n+u_{\alpha/2}^2}\left[\hat{p}+\frac{u_{\alpha/2}^2}{2n}\pm u_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}+\frac{u_{\alpha/2}^2}{4n^2}}\:\right]
$$

A 取负号，B 取正号， $\hat{p}=Y_{n}/n$

由于 $\binom{*}{1}$ 式只是近似成立，故区间估计也只是近似成立，当 n 较大时才相去不远。详细的说明参见课本 p203。我们还可以先假定方差是“已知 " 的，最后再将其估计，得到如下 Wald 置信区间：

$$
\hat{p}\pm u_{\alpha/2}\sqrt{\hat{p}(1-\hat{p})/n}.
$$

### 5.2.2 置信界

在实际中，有时我们只对参数 9 的一端的界限感兴趣。比如果汁的最低含量，有害物质的最高含量等等. 这就需要寻求参数 9 的感兴趣的置信界限

**定义 5.2.2.** 设总体分布 $F (x,\theta)$ 含有一个未知的参数 0， $\theta\in\Theta$ ，对给定的值 $\alpha, (0<\alpha<a)$ 若由样本 $X_{1},\cdots, X_{n}$ 确定的两个统计量 $\bar{\theta}=\bar{\theta}(X_{1},\cdots, X_{n})$ 和 $\underline{\theta}=\underline{\theta}(X_{1},\cdots, X_{n})$

1. 若

$$
P_\theta(\theta\leq\bar{\theta})\geqq1-\alpha\quad\forall\:\theta\in\Theta
$$

则称 $\bar{\theta}$ 为 9 的一个置信系数为 $1-\alpha$ 的置信上界。

2. 若

$$
P_\theta(\theta\geq\underline{\theta})\geqq1-\alpha\quad\forall\:\theta\in\Theta
$$

则称 0 为 0 的一个置信水平为 $1-\alpha$ 的置信下界。

而 $(-\infty,\bar{\theta}]$ 和 $[\underline{\theta},+\infty)$ 都称为是单边的置信区间。

寻求置信上、下界的方法和寻求置信区间的方法完全类似。

### 5.2.3 确定样本大小

在以区间长度为精度准则下，置信区间越窄就越好，为什么呢？作为一个一般的原则，我们已经知道更多的测量可以得到更精确的推断。有时候，对精度是有要求的，甚至于是在测量之前就提出此要求，因此相应的样本大小就要事先确定下来。我们以如下的例子说明如何确定样本大小，一般的方法类似。

例 5.2.4. 假设某种成分的含量服从正态分布 $N (\mu,\sigma^{2})$ ， $\sigma^{2}$ 已知。要求平均含量μ的 $(1-\alpha)$ 置信区间的长度不能长于 $\omega$ 。试确定测量样本大小。

解：由于 $\sigma^2$ 已知，我们已经知道可以根据 $\bar{X}\sim N (\mu,\sigma^{2}/n]$ 来构造 $\mu$ 的 $195\%$ 置信区间。因此易知区间长度为 $2u_{\alpha/2}\frac{\sigma}{\sqrt{n}}$ ．从而由

$$
2u_{\alpha/2}\frac{\sigma}{\sqrt{n}}\leq\omega
$$

得到

$$
n\ge\left(\frac{2u_{\alpha/2}\sigma}{\omega}\right)^{2}.
$$

比如当 $\sigma=0.1,\omega=0.05,\alpha=0.05$ ，可以得到 $n\geq\left (\frac{2\times1.96\times0.1}{0.05}\right)^{2}=61.4656.$ 、即为达到要求至少需要测量 62 次。

[[6 假设检验]]
