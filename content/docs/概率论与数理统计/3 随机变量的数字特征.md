---
title: '3 随机变量的数字特征'
date: 2025-07-10T16:07:38+08:00
math: true
---

1. 理解随机变量的数学期望、方差的概念，并会运用它们的基本性质计算具体分布的期望、方差
2. 掌握二项分布、Poisson 分布、均匀分布、指数分布、正态分布的数学期望和方差
3. 会根据随机变量的概率分布计算其函数的数学期望
4. 理解协方差、相关系数的概念，掌握它们的性质，并会利用这些性质进行计算，了解矩的概念
5. 理解大数定理与中心极限定理。
## 数学期望（均值) 及中位数

### 数学期望

数学期望也称均值，是随机变量的一个最基本的数字特征. 我们先看如下的一个例子

例 3.1.1. 一甲乙两人**赌技相同**，各出赌金 100 元，约定**先胜三局者**为胜，取得全部 200 元。现在甲胜 2 局乙胜 1 局的情况下中止，问**赌本该如何分**？[^1]

解：如果继续赌下去而不中止，则甲有 3/4 的概率取胜，而乙胜的概率为 1/4.[^2] 所以，在甲胜 2 局乙胜 1 局的这个情况下，甲能期望“得到”的数目，应当确定为

$$
200\times\frac{3}{4}+0\times\frac{1}{4}=150(\text{元}),
$$

而乙能“期望 " 得到的数目，则为

$$
200\times\frac{1}{4}+0\times\frac{3}{4}=50(\text{元}).
$$

如果引进一个随机变量 $X$ ， $X$ 等于在上述局面 (甲值 2 胜乙 1 胜) 之下，继续赌下去甲的最终所得，则 $X$ 有两个可能的值：200 和 0，其概率分别为 3/4 和 1/4. 而甲的期望所得即 $X$ 的“期望 " 值，即

$$
X\text{的期望}=X\text{的可能值与其概率之积的累加}
$$

这就是“数学期望”这个名称的由来. 另一个名称“均值”形象易懂，也很常用．下面我们就给出数学期望（均值）的定义

对一般的离散型分布，我们有

**定义 3.1.1.** 设 $X$ 为一离散型随机变量，其分布律为

$$
P(X=x_i)=p_i,\quad i=1,2,\cdots 
$$

如果 $\sum_{i=1}^{\infty}|x_{i}|p_{i}<+\infty$, 则称

$$
\sum_{i=1}^{\infty}x_ip_i
$$

为随机变量 $X$ 的数学期望 (均值), 用符号 $EX$ 表示

如果 $\sum _{i= 1}^{\infty }| x_{i}| p_{i}$ = $+ \infty$, 则称 $X$ 的数学期望 (均值) 不存在

对连续型随机变量，其数学期望的定义如下

**定义 3.1.2.** 如果连续型随机变量 X 具有密度函数 $f(x)_{:}$ 则当

$$
\int_{-\infty}^{\infty}|x|f(x)dx<\infty 
$$

时，我们将积分

$$
\begin{aligned}\int_{-\infty}^{\infty}xf(x)dx\end{aligned}
$$

的值称为 X 的数学期望，记作 $EX$ .如果

$$
\int_{-\infty}^{\infty}|x|f(x)dx=\infty,
$$

则称 $X$ 的数学期望不存在.

下面求解几种常见分布的数学期望 [[2 随机变量及其分布]]

1. 二项分布 $X\sim B(n,p)$

$$
\begin{aligned}EX&=\quad\sum_{k=0}^{n}k.\frac{n!}{k!(n-k)!}p^{k}(1-p)^{n-k}\\&=\quad np.\sum_{i=0}^{n-1}\frac{(n-1)!}{i!(n-1-i)!}p^{i}(1-p)^{n-1-i}\\&=\quad np\end{aligned}
$$

2. Poisson 分布 $X\sim P(\lambda)$

$$
EX=\lambda 
$$

3. 正态分布 $X\sim N(\mu,\sigma^{2})$

$$
\begin{aligned}EX&=\quad\int_{-\infty}^{+\infty}\frac{x}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}dx\\&=\quad\int_{-\infty}^{+\infty}(\sigma y+\mu).\frac{1}{\sqrt{2\pi}}e^{-y^{2}/2}dy\\&=\quad\mu\end{aligned}
$$

4. 均匀分布 $X\sim U[a,b]:$

$$
EX=\frac{a+b}{2}
$$

5. 指数分布 $X\sim Exp(\lambda)$

$$
EX=1/\lambda 
$$

### 数学期望的性质

1. 若干个**随机变量线性组合**的期望，等于各变量期望的线性组合. 假设 $c_{1},c_{2},\ldots,c_{n}$ 为常数，则有 [^3]

$$
E(c_1X_1+c_2X_2+\cdots+c_nX_n)=c_1EX_1+c_2EX_2+\cdots+c_nEX_n,
$$

**例** 3.1.2. 假设随机变量 $X\sim B(n,p)$, 求 $EX$

**解**：令 $I_{i}\sim B( 1, p)$, $i= 1, 2, \ldots , n$, 则 $X=\sum_{i=1}^{n}I_{i}$ 且 $EI_i=p$ ．所以， $EX=\sum_{i=1}^{n}EI_{i}=np$

2. 若干个**独立随机变量之积**的期望，等于各变量的期望之积. 即

$$
E(X_1X_2\cdots X_n)=EX_1EX_2\cdots EX_n,
$$

这里假定各变量相互独立且期望都存在

3.（**随机变量函数的期望**）设随机变量 $X$ 为离散型，有分布 $P( X$ = $a_{i})$ = $p_{i}$, $i$ = i= $i=$ $1,2,\ldots$, 或者为连续型，有概率密度函数 $f(x)$. 考虑一个函数 g (x)[^4], 对应的期望为

$$
Eg(X)=\left\{\begin{array}{ll}\sum_ig(a_i)p_i,&\sum_i|g(a_i)|p_i<\infty;\\\int_{-\infty}^{+\infty}g(x)f(x)dx,&\int_{-\infty}^{+\infty}|g(x)|f(x)dx<\infty.\end{array}\right.
$$

例 3.1.3. 假设 c 为常数，则 $EcX=cEX$

例 3.1.4. 设随机变量 $X\sim N(0,1)$ ，求 $Y=X^{2}+1$ 的数学期望，

解：由 $X\sim N(0,1)$ ，正态分布，连续型，使用积分形式, $g(x)=x^2$

$$
\begin{array}{rcl}EX^2&=&\displaystyle\int_{-\infty}^{+\infty}x^2.\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx\\&=&1.\end{array}
$$

所以， $EY=EX^{2}+1=2$[^5]

例 3.1.5. 飞机场载客汽车上有 20 位乘客，离开机场后共有 10 个车站可以下车，若某个车站没有人下车则该车站不停车，设乘客在每个车站下车的可能性相等，以 X 表示停车的次数，求 $EX$

解：设

$$
Y_i=\left\{\begin{array}{ll}1,&\text{第}i\text{个车站有人下车}\\0,&\text{第}i\text{个车站无人下车}\end{array}\right.\quad i=1,\cdots,20.
$$

则显然 $X=\sum_{i=1}^{20}Y_i$,[^6] 所以

$$
\begin{aligned}EX&=\quad\sum_{i=1}^{20}EY_{i}=\sum_{i=1}^{20}P(\text{第 }i\text{ 个车站有人下车})\\&=\quad\sum_{i=1}^{20}[1-0.9^{20}]=8.784.\end{aligned}
$$

### 条件期望

我们知道条件分布也是一个概率分布，因此类似数学期望的定义，我们可以给出条件期望的定义. 在**给定了随机变量 $X$ 取值 $x$ 的条件之下**， $Y$ 的条件期望，我们记为 $E (Y|X=x)$，也可简记为 $E (Y|x)$

**定义 3.1.3.** 设 $X$ 和 $Y$ 为随机变量，若 $(X, Y)$ 为离散型，且在给定 $X=x$ 之下， $Y$ 有分布 $P (Y=$ $a_{i}| X= x) = p_{i}$, $i= 1, 2, \ldots$, ，或者 $(X, Y)$ 为连续型，且在给定 $X=x$ 之下， $Y$ 的条件密度函数为 $f (y|x)$, 则

$$
E(Y|X=x)=\left\{\begin{array}{ll}\int_{-\infty}^{+\infty}yf(y|x)dy,&(X,Y)\text{为连续型};\\\sum_{i}a_ip_i,&(X,Y)\text{为离散型}.\end{array}\right.
$$

> [!note]
**期望所具有的性质, 条件期望同样满足**

例 3.1.6. 设 $(X, Y)\sim N (a, b,\sigma_{1}^{2},\sigma_{2}^{2},\rho)$ ，试计算 $E (Y|X=x)$

解：由于 $Y \mid X = x \sim N\left (b + \rho \frac{\sigma_{2}}{\sigma_{1}}(x - a),\ (1 - \rho^{2})\sigma_{2}^{2}\right)$, 所以由 [二维正态分布](00 补充知识/二维正态分布) 的性质知

- [ ] 没整明白

$$
E(Y|X=x)=b+\rho\frac{\sigma_2}{\sigma_1}(x-a).
$$

条件期望 $E (Y|X=x)$ 是 $x$ 的函数，当我们将 $x$ 换为 $X$ 时， $E (Y|X)$ 就是一个随机变量

**定理 3.1.1.** 设 $X, Y$ 为两个随机变量，则有 [^7]

$$
EX=E\{E[X|Y]\} \tag{全期望公式}
$$

证：我们仅在连续型随机变量的情形下证明此定理. 设 $Y$ 的 p.d.f 为 $p (y)$，$X|Y=y$ 的 p.d.f 为 $q (x|y)$，则

$$
\begin{aligned}EX&=\quad\iint_{-\infty}^{\infty}q(x|y)p(y)dxdy\\&=\quad\int\int_{-\infty}^{\infty}q(x|y)dxp(y)dy=\int_{-\infty}^{\infty}E[X|Y=y]p(y)dy\\&=\quad E\{E[X|Y]\}\end{aligned}
$$

**推广**：当 $g (X)$ 为可积随机变量时，有 $Eg (X)=E\{E[g (X)|Y]\}$

由此得到求解期望的第二种方法：先求解 $h (x)=E (Y|X=x)$ ，再求解 $Eh (X)$ ，即可求得 $EY$

---

例 3.1.7. 一窃贼被关在有 3 个门的地牢里，其中第一个门通向自由．出这门走 3 个小时便可以回到地面；第 2 个门通向另一个地道，走 5 个小时将返回到地牢；第 3 个门通向更长的地道，走 7 个小时也回到地牢，若窃贼每次选择 3 个门的可能性总相同，求他为获得自由而奔走的平均时间。[好题](01 好题/_index)

> 要求的是时间，设时间为随机变量

解：设这个窃贼需要走 $X$ 小时才能到达地面，并设 $Y$ 代表他每次对 3 个门的选择情况， $Y$ 各以 1/3 的概率取值 1,2,3. 则

$$
EX=E[E(X|Y)]=\sum_{i=1}^3E(X|Y=i)P(Y=i)
$$

$E (X|Y=1)=3, E (X|Y=2)=5+EX, E (X|Y=3)=7+EX$，而 $P (Y=i)=1/3$

$$
EX=\frac{1}{3}(3+5+EX+7+EX)
$$

解得

$$
EX=15
$$

例 3.1.8. 设 $(X, Y)\sim N (a, b,\sigma_{1}^{2},\sigma_{2}^{2},\rho)$ ，试计算 $EXY$

解：先算得

$$
E(XY|X=x)=xE(Y|X=x)=x(b+\rho\frac{\sigma_2}{\sigma_1}(x-a));
$$

所以

$$
\begin{aligned}
EXY& =\quad E(bX+\rho\frac{\sigma_{2}}{\sigma_{1}}X^{2}-\rho\frac{\sigma_{2}}{\sigma_{1}}aX) \\
&=\quad ab+\rho\frac{\sigma_{2}}{\sigma_{1}}(a^{2}+\sigma_{1}^{2})-\rho\frac{\sigma_{2}}{\sigma_{1}}a^{2} \\
&=\quad ab+\rho\sigma_{1}\sigma_{2}.
\end{aligned}
$$

### 中位数

我们已经知道，随机变量 $X$ 的数学期望就是它的平均值，因此从一定意义上，数学期望刻画了随机变量所取之值的“中心位置”. 但是，我们也可以用别的数字特征来刻画随机变量的“中心位置”。中位数就是这样一种数字特征

**定义 3.1.4.** 称 $\mu$ 为连续型随机变量 X 的中位数，如果

$$
P(X\leq\mu)=\frac{1}{2},\quad P(X\geq\mu)=\frac{1}{2}.
$$

从定义上可以看出， $m$ 这个点把 $X$ 的分布从概率上一分两半：在 $m$ 左边占一半， $m$ 右边也占一半，从概率上说， $m$ 这个点正好居于中央，这就是“中位数”得名的由来. 在实用上，中位数用得很多，特别有不少社会统计资料，常拿中位数来刻化某种量的代表性数值，有时它比数学期望更说明问题，例如，某社区内人的收入的中位数告诉我们：有一半人的收入低于此值，另一半高于此值. 我们直观上感觉到这个值对该社区的收入情况的确很具有代表性，和期望值相比它的一个优点是受个别特别大或特别小的值的影响很小，而期望则不然，举例而言，若该社区中有一个收入在百万元以上，则该社区的均值可能很高，而绝大多数人并不富裕，这个均值并不很有代表性，中位数则不然，它几乎不受少量这种特大值的影响 [^8]

从理论上说，中位数与均值相比还与一个优点，即它总存在，而均值则不是对任何随机变量都存在. 虽则中位数有这些优点，但在概率统计中，无论理论和应用上，数学期望的重要性都超过中位数，其原因有一下两个方面：

1. 均值有很多优良的性质，这些性质时使得在数学处理上很方便. 例如，$E (X_{1}+X_{2})=EX_1+EX_2$ ，而 $X_1+X_2$ 的中位数与 $X_1, X_{2}$ 的中位数之间，不存在这样简单的联系。这使中位数在数学上的处理很复杂且不方便

2. 中位数本身固有的某些缺点：中位数可以不唯一，且对于离散型随机变量不易定义

例 3.1.9. 设随机变量 $X\sim B (1,\frac{1}{2})$, 求 X 的中位数 [好题](01 好题/_index)

解：由于 X 的分布函数为

$$
F(x)=\begin{cases}&0,\quad x\leq0\\&\frac{1}{2},\quad0<x<1\\&1,\quad x\geq1\end{cases}
$$

由中位数的定义知区间 (0,1) 内的每一个数都是 $X$ 的中位数，所以此例说明中位数可以不唯一.

## 方差、标准差和矩

### 方差和标准差

现在我们转到本章开始时候提到的另一类数字特征，即刻画随机变量在其中心位置附近散布程度的数字特征，其中最重要的是方差．在实际应用中，方差不仅是**信息度量的标准**也是风险度量的标准

**定义 3.2.1.** 设 $X$ 为随机变量，分布为 $F$ ，均值为 $\mu$, $X$ (或分布 $F$) 的方差

$$
 \sigma^2=\mathrm{Var}(X)\equiv E(X-\mu)^2\tag{方差}
$$

$$
\mathrm{Var(X)}=EX^2-\mu ^2.
$$

$$
\sigma=\sqrt{Var (X)}\tag{标准差}
$$

**定理 3.2.1.** 设 c 为常数. 则有

1. $0\leq Var (X)=EX^{2}-(EX)^{2}$ 因此 $Var (X)\leq EX^{2}$

2. $Var (cX)=c^{2}Var (X)$

3. $Var (X)=0$ 当且仅当 $P (X=c)=1$, 其中 $c=EX$, 此时称 X 退化到常数 c.

4. $Var (X)\leq E (X-c)^{2}$, 其中等号成立当且仅当 $c=EX.$

5. 如果随机变量 $X$ 和 $Y$ 相互独立， $a, b$ 为常数. 则 $Var (aX+bY)=a^{2}Var (X)+b^{2}Var (Y).$

#### 常见分布的方差

1. 二项分布 $X\sim B (n, p)$

$$
VarX=np(1-p)
$$

2. Poisson 分布 $X\sim P (\lambda)$

$$
VarX=\lambda 
$$

3. 均匀分布 $X\sim U[a, b]:$

$$
VarX=\frac{(b-a)^2}{12}
$$

4. 指数分布 $X\sim Exp (\lambda)$

$$
VarX=1/\lambda^2
$$

5. 正态分布 $X\sim N (\mu,\sigma^{2})$

$$
VarX=\sigma^2
$$

由此得到正态分布 $N (\mu,\sigma^2)$ 中另一参数 $\sigma^2$ 的解释：它就是分布的方差，正态分布完全由其均值 $.\mu$ 和方差 $\sigma^2$ 决定，故也常称为“均值为 $\mu$ 方差为 $\sigma^2$ 的正态分布 ". 方差 $\sigma^2$ 越小，则 $X$ 的取值以更大的概率集中在其均值 $\mu$ 附近

#### 标准化随机变量

**定义 3.2.2.** 我们称

$$
X^*=\frac{X-EX}{\sqrt{Var(X)}}
$$

为 $X$ 的标准化随机变量. 易见 $EX^{*}=0, Var (X^{*})=1$

> [!note]
> 这里先考虑分子的减法，$X^*=X-EX$ 仅变动了均值，方差并未改变，再考虑除法，同时改变了均值和方差（由于均值已经变成了 0，所以无影响）

标准化的目的：我们引入标准化随机变量是为了消除由于计量单位的不同而给随机变量带来的影响. 例如，我们考察人的身高，那么当然可以以米为单位，得到 $X_1$ ，也可以以厘米为单位，得到 $X_2$ ．于是就有得到 $X_{2}=100X_{1}$ ．那么这样一来， $X_{2}$ 与 $X_1$ 的分布就有所不同这当然是一个不合理的现象. 但是通过标准化，就可以消除两者之间的差别，因为我们有 $X_{2}^{*}=X_{1}^{*}$ ．对于正态分布，我们经过标准化 $Y$ = $( X$ $- \mu ) / \sigma$ ，就可以得出均值为 0 方差为 1 的正态分布，即标准正态分布

### 矩

下面我们引入矩的概念，并将之与我们前面所说的期望、方差建立联系

**定义 3.2.3.** 设 $X$ 为随机变量， $c$ 为常数， $r$ 为正整数，则 $E[(X-c)^{r}]$ 称为 $X$ 关于 c 点的 $r$ 阶矩.

比较重要的有两个情况

1. **原点矩**：$c=0$ .这时 $\alpha_{k}=EX^{r}$ 称为 $X$ 的 $r$ 阶**原点矩**

2. **中心矩**：$c=EX$ .这时 $\mu_{k}=E[(X-EX)^{r}]$ 称为 $X$ 的 $r$ 阶**中心矩**

容易看出，**一阶原点矩就是期望，二阶中心矩就是 $X$ 的方差** $Var (X)$

## 协方差和相关系数（多维随机向量）

现在我们来考虑**多维随机向量的数字特征**，以二维的情况为例，设 $(X, Y)$ 为二维随机变量， $X,Y$ 本身都是一维随机变量，那么它们相应的均值方差，我们都在上两节中讨论过了，我们更有兴趣的数字特征是反映分量之间关系的那种量，其中最重要的，是本节要讨论的**协方差和相关系数**

### 协方差

**定义 3.3.1.** 我们称

$$
\sigma (X, Y)=\mathrm{Cov}(X,Y)=E(X-EX)(Y-EY)
$$

为 $X$ 与 $Y$ 的协方差，其中 Cov 是英文单词 Covariance 的缩写.

由协方差的定义，我们立刻可以得到协方差具有如下性质：[^9]

1. $Cov (X, Y)=Cov (Y, X),\: Cov (X, X)=Var (X)$

2. $Cov (X, Y)=EXY-EXEY$, 显然**若 $X, Y$ 相互独立，则 $Cov (X, Y)=0$**

3. $Cov (X_1+X_2, Y)=Cov (X_1, Y)+Cov (X_2, Y)$

4. (双线性) 对任何实数 $a_{1}, a_{2}, b_{1}, b_{2}$, 有

$$
Cov(a_1X_1+a_2X_2,b_1Y_1+b_2Y_2)=\sum_{i=1}^2\sum_{j=1}^2a_ib_jCov(X_i,Y_j)
$$

特别的，$Var (cX)=Cov (cX, cX)=c^2 Cov (X, X)=c^2 Var (X)$

### 相关系数

**定义 3.3.2.** 设随机变量 $X, Y$ 为随机变量，称

$$
\rho_{X,Y}=\frac{Cov(X,Y)}{\sqrt{VarX}\cdot\sqrt{VarY}}
$$

为 $X$ 与 $Y$ 的相关系数. 当 $\rho_{X, Y}=0$ 时，则称 $X$ 与 $Y$ 不相关.

由定义容易看出，若令 $X^{* }$ = $( X$ - $EX) / \sqrt {VarX}$ 和 $Y^{* }$ = $( Y$ - $EY) / \sqrt {VarY}$ 分别为 $X$ 和 $Y$ 相应的标准化随机变量，则

$$
\rho _{X, Y} = Cov ( X^{* }, Y^{* })
$$

因此，形式上**可以把相关系数视为“标准尺度下的协方差”**，从这个角度上说，相关系数可以更好的反映两个随机变量间的关系，而不受它们各自所用度量单位的影响

例 3.3.1. 设 $(X, Y)\sim N (a, b,\sigma_{1}^{2},\sigma_{2}^{2},\rho)$ ，则 $\rho_{X, Y}=\rho$

---

**相关系数有如下的性质**

1. 若 $X$ 和 $Y$ 相互独立，则 $\rho_{X, Y}=0$ (因为此时 $\mathrm{Cov}(X,Y)=E(X-EX)(Y-EY)=0$)

2. $|\rho_{X, Y}|\leq1$ ，等号成立当且仅当 $X, Y$ 之间存在**严格的线性关系**，即

$$
\rho_{X, Y}=1,\text{则存在} a>0, b\in\mathbb{R} \text{使得} X=aY+b \tag{正相关}
$$

$$
\rho_{X, Y}=-1, \text{则存在} a<0, b\in\mathbb{R} \text{使得} X=aY+b \tag{负相关}
$$

[注]： $\rho_{X, Y}$ 也常称作 $X$ 和 $Y$ 线性相关系数，只能刻画 $X$ 和 $Y$ 间的线性相依程度， $|\rho_{X, Y}|$ 越接近 1，就表示 $X,Y$ 间的线性相关程度越高： $|\rho_{X, Y}|=0$ 时，**只是表示 $X$ 和 $Y$ 间不存在线性相关，但可以存在非线性的函数关系**

例 3.3.2. 设 $X\sim U ( - \frac 12$, $\frac 12)$ ,均值为 0, 而 $Y=cosX$ ,则

$$
\begin{aligned}
\text{Cov}(X, Y) &= E[XY] - E[X]E[Y] \\
&= E[XY] - 0 \cdot E[Y] \\
&= E[XY] \\
&= \int_{-\frac{1}{2}}^{\frac{1}{2}} x \cos(x) \, dx \\
&= 0
\end{aligned}
$$

所以 $X, Y$ 不相关. 但是 $X, Y$ 之间存在着非线性的函数关系。

### 不相关与独立性之间的关系

**定理 3.3.1.** 对随机变量 $X, Y$

1. 独立一定不相关

2. 不相关无法判断独立性

例 3.3.3. 试证明若 $(X, Y)$ 服从单位圆内的均匀分布，则 $X, Y$ 不相关但不独立

解：由 $(X, Y)$ 服从单位圆内的均匀分布，则 $(X, Y)$ 的联合密度函数

$$
f(x,y)=\left\{\begin{array}{ll}\frac{1}{\pi},&x^2+y^2\leq1;\\0,&\text{其他}.\end{array}\right.
$$

由此，可得 $X$ 和 $Y$ 的边缘密度函数为

$$
f_X(x)=f_Y(x)=\frac{2}{\pi}\sqrt{1-x^2},\quad-1\le x\le1.
$$

因此, $EX=EY=0$ ,又

$$
EXY=\int_{-1}^{1}x.\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}y.\frac{1}{\pi}dydx=0.
$$

所以， $Cov (X, Y)=0$, 从而 $\rho_{X, Y}=0$, 即 $X$ 和 $Y$ 不相关. 但由 $f (x, y)\neq f_{X}(x). f_{Y}(y)$, 知 X 和 Y 显然不独立

例 3.3.4. 设随机变量 $X$ 和 $Y$ 的分布律分别为

$$
X\sim\left(\begin{array}{ccc}-1&0&1\\\frac{1}{4}&\frac{1}{2}&\frac{1}{4}\end{array}\right),\quad Y\sim\left(\begin{array}{ccc}0&1\\\frac{1}{2}&\frac{1}{2}\end{array}\right)
$$

并且 $P (X\cdot Y=0)=1$. (这里是题目条件)

则 $X$ 与 $Y$ 不独立，也不相关

[注]：**只在正态情形下，不相关与独立等价**. 我们举二维正态的例子来说明，不妨设 $(X, Y)\sim N (a, b,\sigma_{1}^{2},\sigma_{2}^{2},\rho)$ ，则 $X$ 和 $Y$ 独立等价于 $\rho=\rho_{X, Y}=0$ ，从而等价于 $X$ 和 $Y$ 不相关

## 其他一些数字特征与相关函数

- 平均绝对差 $E|X-EX|$

表 3.3.1 常见分布表

![](https://image.huarenjian.cn/image/Pasted%20image%2020250711223248.png)

---

### 疑似复分布问题 - 总之先了解

**定义 3.4.1.** 如果离散型随机变量 $X$ 的分布律为 $P ( X= a_{i}) = p_{i}$, $i\in \mathbb{N}$, ，那么

$$
Ee^{itX}=\sum_{i=1}^{\infty}e^{ita_i}p_i.
$$

如果连续型随机变量 X 的密度函数为 $f (x)$ ，那么

$$
Ee^{itX}=\int_{-\infty}^{\infty}e^{itx}f(x)dx.
$$

## 大数定律和中心极限定理

极限定理是概率论的重要内容，也是数理统计学的基石之一. 中心极限定理，是概率论中讨论随机变量和的分布以正态分布为极限的一组定理这组定理是数理统计学和误差分析的理论基础，指出了大量随机变量近似服从正态分布的条件

### 大数定律

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=466757415&bvid=BV1KL411T7rG&cid=516507270&p=17&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>

**定义 3.5.1.** 如果对任何 $\varepsilon>0$ ，都有

$$
\lim_{n\to\infty}P(|\xi_n-\xi|\geq\varepsilon)=0,
$$

那么我们就称随机变量序列 $\xi _n$, $n\in \mathbb{N}$ 依概率收敛到随机变量 $\xi$，记为 $\xi_{n}\stackrel{p}{\longrightarrow}\xi.$

**定理 3.5.1.** 设 $\{X_n\}$ 是一列独立同分布 $(i.i.d.)$ 的随机变量序列 [^10]，具有公共的数学期望 $\mu$ 和方差 $\sigma^{2}$ ：则

$$
\overline{X}=\frac{1}{n}\sum_{k=1}^{n}X_{k}\xrightarrow{p}\mu,
$$

即 $\{X_n\}$ 服从 (弱) 大数定律。

[注]：实际上，我们只需要均值存在即有大数定律成立，上述定理中加上了方差存在的条件，只是为了证明的方便。

> [!note] 补充
  [知乎-独立和同分布的解释](https://www.zhihu.com/question/21418108)

作为上述定理的一个特例，我们有

例 3.5.1. 如果以 $\zeta_n$ 表示 $n$ 重 Bernoulli 试验中的成功次数，则有

$$
\frac{\zeta_n}{n}\xrightarrow{p}p.
$$

如果用 $f_{n}=\zeta_{n}/n$ 表示成功出现的频率，则上例说明 $f_{n}\xrightarrow{p}p$ 即频率（依概率）收敛到概率.

为证明**定理 3.5.1.** 我们需要如下的 Chebyshev 不等式

**引理 3.5.1**（Chebyshev 不等式）. 设随机变量 $X$ 的方差存在，则

$$
P(|X-EX|\geq\varepsilon)\leq\frac{Var(X)}{\varepsilon^{2}},\quad\forall\:\varepsilon>0.
$$

> [!note]
我们可以用 Chebyshev 不等式来估计 $X$ 与 $EX$ 的偏差，但是 Chebyshev 不等式作为一个理论工具比作为估计的实际方法要恰当一些，其重要性在于它的应用普遍性，但是不能希望很普通的命题对一些个别情况给了深刻的结果. 如令 $X$ 为掷一个均匀的般子所得到的点数, 则 $\mu = EX= 7/ 2$, $\sigma ^{2}=$ Var $(X)=35/12$ . $X$ 与 $\mu$ 的最大偏差为 $2. 5\approx 3\sigma / 2.$ $|X-\mu|$ 大于这个偏差的概率为 0. 然而利用 Chebyshev 不等式仅仅断定这个概率少于 0.47. 这时就需要找更精确的估计.

定理 3.5.1 的证明. 利用 Chebyshev 不等式，并注意到 $E\overline{X}=\mu$, Var $\overline X=\sigma^2/n$ 我们有，

$$
P(|\overline{X}-\mu|\geq\varepsilon)\leq\sigma^{2}/(n\varepsilon^{2})\to0,\:n\to\infty,\:\forall\varepsilon>0.
$$

定理得证

### 中心极限定理

中心极限定理是概率论中讨论随机变量序列的分布收敛于正态分布的一类定理. 它是概率论中最重要的一类定理，有广泛的实际应用背景. 在自然界与生产中，一些随机现象可能会受到许多不确定因素的影响，如果这些彼此之间没有什么依存关系，且谁也没有特别突出的影响，那么这些**影响的累积效应将会使现象近似地服从正态分布**. 中心极限定理就是从数学上证明了这一现象

**定理 3.5.2.** 设 $\{X_n\}$ 为 i.i.d 的随机变量序列，具有公共的数学期望 $\mu$ 和方差 $\sigma^2$ ，则 $X_1+$ $\cdots+X_{n}$ 的标准化形式 [^11] $\frac{1}{\sqrt{n}\sigma}(X_{1}+\cdots+X_{n}-n\mu)$ 满足中心极限定理. 即对任意 $x\in\mathbb{R}$ ，有

$$
\lim_{n\to\infty}F_n(x)=\Phi(x),
$$

其中 $F_n (x)$ 为 $\frac{1}{\sqrt{n}\sigma}(X_{1}+\cdots+X_{n}-n\mu)$ 的分布函数，而 $\Phi (x)$ 为标准正态分布 $N (0,1)$ 的分布函数. 记为

$$
\frac{1}{\sqrt{n}\sigma}(X_1+\cdots+X_n-n\mu)\xrightarrow{d}N(0,1).
$$

**定理 3.5.2 的令人吃惊之处就是任何独立同分布的随机变量序列，不论它的分布是什么. 只要存在有限的方差那么它们的标准化部分和都渐近于标准正态分布这也说明了正态分布的普遍性**

由**定理 3.5.2.** 我们很容易得到如下推论

**定理 3.5.3.** 设 $X_{1},\cdots, X_{n}$ 相互独立且具有相同的分布

$$
P(X_1=1)=1-P(X_1=0)=p,\:0<p<1.
$$

则有

$$
\begin{aligned}\frac{X_1+\cdots+X_n-np}{\sqrt{np(1-p)}}\xrightarrow{d}N(0,1).\end{aligned}
$$

即

$$
\lim\limits_{n\to\infty}P\Big(\frac{X_1+\cdots+X_n-np}{\sqrt{np(1-p)}}\le x\Big)=\Phi(x),\quad\forall\:x\in\mathbb{R}.
$$

定理 3.5.2 称为棣募弗 - 拉普拉斯定理. 是历史上最早的**中心极限定理**. 因为定理 3.5.2 中随机变量 $X_{1},\cdots, X_{n}$ 的和 $X_{1}+\cdots+X_{n}\sim B (n, p)$ ，我们**利用正态分布近似地估计二项分布**.

- [ ] 这里没看懂

设 $t_1<t_2$ 是两个正整数，则当 $n$ 相当大时，由**定理 3.5.2.** 近似地有

$$
P(t_1\leq X_1+\cdots+X_n\leq t_2)\approx\Phi(y_2)-\Phi(y_1),
$$

其中

$$
y_i=(t_i-np)/\sqrt{np(1-p)},\:i=1,2.
$$

为提高精度，我们可把 $y_{1}, y_{2}$ 修正为

$$
y_1=(t_1-1/2-np)/\sqrt{np(1-p)},\quad y_2=(t_2+1/2-1/2-np)/\sqrt{np(1-p)}.
$$

例 3.5.2. 设一考生参加 100 道题的英语标准化考试（每道题均为有四个备选答案的选择题，有且仅有一个答案是正确的），每道题他都随机地选择一个答案，假设评分标准为： 选对得一分，选错或不选不得分。试给出该考生最终得分大于等于 25 的概率

解：记 $X_i$ 表示第题的得分， $i$ = 1,2 $\cdots$ ,100 则 $X_{1},\cdots, X_{n}$ 是一列独立同分布的随机变量具有共同的分布

$$
1-P(X_1=0)=P(X_1=1)=0.25.
$$

利用中心极限定理有

$$
P(X_1+\cdots+X_{100}\ge25)=P\Big(\frac{X_1+\cdots+X_{100}-100*0.25}{\sqrt{100*0.25*0.75}}\ge0\Big)=1-\Phi(0)=1/2.
$$

例 3.5.3. 每天有 1000 个旅客需要乘坐火车从芝加哥到洛杉矶，这两个城市之间有两条竞争的铁路，它们的火车同时开出同时到达并且具有同样的设备. 设这 1000 个人乘坐那一条铁路的火车是相互独立而且又是任意的，于是每列火车的乘客数目可视为概率为 1/2 的 1000 重 Bernoulli 试验中成功的次数. 如果一列火车设置 $s<n$ 个座位，那么一旦有多于 $s$ 个旅客来乘车就容纳不下了，令这个事件发生的概率为 $f (s)$ 利用中心极限定理，有

$$
f(s)\approx1-\Phi\Big(\frac{2s-1000}{\sqrt{1000}}\Big).
$$

要求 s 使得 $f (s)<0.01$ 即在 100 次中有 99 次是有足够的座位的. 查表容易求出 $s=537$ 这样，两列火车所有的座位数为 1074, 其中只有 74 个空位，可见由于竞争而带来的损失是很小的.

[[4 数理统计的基本概念及抽样分布]]

[^1]: hrj: 实际上是求最后获得的期望，但我有个疑问，两方期望的和一定自洽吗？
[^2]: hrj: 这里要考虑后面可能的局数噢
[^3]: 这里的 $X_1, X_2$ 是两个随机量, 不是同一随机量的不同取值，后者通常用小写字母表示
[^4]: 这里 g (x) 需满足什么条件书上没提, 暂且理解为 " 常规 " 形式吧
[^5]: 涉及到的积分公式见 [概率论部分](../暂存/概率论部分)
[^6]: 这里的 $Y_i$ 是指示变量，就是表示随机变量 X 的每种可能，=1 就是发生，=0 就是不发生，同样满足之前的线性规则，即 $E (X)=E (Y_1+Y_2+\cdots+Y_{20})=E (Y_1)+E (Y_2)+\cdots+E (Y_{20})$
[^7]: 仅大致看懂，还不会用
[^8]: 与均值相比，中位数常常反应的是大多数的位置
[^9]: 这些证明写写就挺显然的，因为 E 本身就是线性运算
[^10]: 也就是从 $X_1,X_2,\cdots,X_n$ 的序列，其中每一个都是独立的随机变量
[^11]: 标准化随机变量
